1:Kafka 消费方式
    - Pull 模式:
      consumer 采用从 Broker 中主动拉取数据
    - Push 模式:
      Kafka 没有采用这种模式, 因为由 broker 决定消息发送速率, 很难适应所有消费者的消费速率
      pull 模式不足之处是: 如果 kafka 没有数据, 消费者可能会陷入循环中,一直返回空数据.

2.消费者组原理:
    - Consumer Group(CG) 有多个 consumer 组成, 形成一个消费者组的条件,是所有消费者的 groupid 相同,
    - 消费者组内每个消费者负责消费不同分区的数据,一个分区只能由一个组内消费者消费
    - 消费者组之间互不影响.所有的消费者都属于某个消费者组,即消费者组是逻辑上的一个订阅者.
    - 如果消费者组中的消费者超过主题分区数量,则有一部分消费者就会闲置,不会接收任何消息.


3.生产经验 - 分区的分配以及再平衡
    1.一个 consumer group 中有多个 consumer 组成, 一个 topic 有多个 partition 组成, 现在的问题是,
      到底由哪个 consumer 来消费哪个 partition 的数据.
    2.Kafka 有四种主流的分区分配策略: Range、RoundRobin、Sticky、CooperativeSticky
      可以通过配置参数 partition.assignment.strategy 来修改分区的分配策略. 默认策略是
      Range + CooperativeSticky. Kafka 可以同时使用多个分区分配策略.
    相关参数
    - heartbeat.interval.ms
      Kafka 消费者和 coordinator 之间的心跳时间,默认3s
      该条目的值必须小于 session.timeout.ms, 也不应该高于 session.timeout.ms 的 1/3
    - session.timeout.ms
      Kafka 消费者和 coordinator 之间连接超时时间, 默认45s, 超过该值,该消费者被移除,
      消费者组执行再平衡.
    - max.pool.interval.ms
      消费者处理消息的最大时长, 默认5分钟,超过该值,该消费者被移除,消费者组执行再平衡
    - partition.assignment.strategy
      消费者分区分配策略, 默认策略是 Range + CooperativeSticky. Kafka 可以同时使用多个
      分区分配策略. 可以选择的策略包括: Range, RoundRobin, Sticky, CooperativeSticky


4.分配策略之 Range
    - Range 是对每个 topic 而言的. 首先对同一个 topic 里面的分区按照序号进行排序,并对消费者
      按照字母顺序进行排序.
    - 假如现在有7个分区,3个消费者,排序后的分区将会是 0,1,2,3,4,5,6, 消费者排序完后将会是c0,c1,c2
    - 通过 partitions / consumer 来决定哪个消费者应该消费哪几个分区. 如果除不尽,那么前面
      几个消费者将会多消费一个分区.
    - 例如: 7/3 = 2 余 1, 除不尽, 那么消费者C0便会多消费一个分区. 8/3=2余2,除不尽,那么 
      C0 和 C1 分别多消费一个.
    - 注意: 如果只是针对一个 topic 而言, C0 消费者多消费1个分区影响不大. 但是如果N个 topic,
      那么针对每个 topic, 消费者 C0 都将多消费1个分区, topic 越多, C0 消费的分区会比其他
      消费者明显多 N 个分区.
      容易产生数据倾斜.
    
    kafka-topics.sh --bootstrap-server kafka01:9092 --topic first --alter --partitions 7
    kafka-topics.sh --bootstrap-server kafka01:9092 --topic first --describe

5.分配策略之 RoundRobin
    - RoundRobin 针对集群中所有 Topic 而言
    - RoundRobin 轮训分配策略, 是把所有的 partition 和所有的 consumer 都列出来, 然后按照
      hashcode 进行排序,最后通过轮训算法来分配 partition 给各个消费者.

6.分配策略之 Sticky 以及再平衡
    - 粘性分区定义: 可以理解为分配的结果带有 “粘性”, 即在执行一次新的分配之前,考虑上一次
      分配的结果. 尽量少的调整分配的变动,可以节省大量的开销.
    - 粘性分配是 Kafka 从 0.11.x 版本开始引入这种分配策略. 首先会尽量均衡的放置分区到消费者
      上面. 在出现同一消费组内消费者出现问题的时候,会尽量保证原有分配的分区不变化.


7.offset 的默认维护位置
    - 从 0.9 版本开始, consumer 默认将 offset 保存在 Kafka 一个内置的 topic 中, 该 topic
      为 __consumer_offsets. 0.9 版本之前, consumer 默认将 offset 保存在 zookeeper 中.
    - __consumer_offsets 主题主题采用 key-value 的方式存储数据. key 是 group_id + topic + 分区号,
      value 是当前 offset 的值. 每隔一段时间, kafka 内会对这个 topic 进行 compact,
      也就是每个 group.id + topic + 分区号就保留最新数据.
    // 查看消费者消费主题 __consumer_offsets


8.自动提交 offset
    - 为了使我们能够专注于自己的业务逻辑, Kafka 提供了自动提交 offset 的功能.
    - 自动提交 offset 的相关参数
        enable.auto.commit          是否开启自动提交 offset 功能, 默认是 true
        auto.commit.interval.ms     自动提交 offset 的时间间隔, 默认是 5s

9.手动提交 offset
    - 虽然自动提交 offset 简单便利,但是其是基于时间提交的.开发人员难以把握 offset 提交的
      时机. 因此 kafka 还提供了手动提交 offset 的 API
    - 手动提交 offse 的两种方法有两种: 分别是 commitSync(同步提交) 和 commitAsync(异步提交),
      两者的相同点是, 都会将本次提交的一批数据最高的偏移量提交,不同的是,同步提交阻塞当前
      线程,一直到提交成功,并且会自动失败重试(由不可控因素导致,也会出现提交失败);
      而异步提交则没有失败重试机制,故有可能提交失败.
    - commitSync(同步提交):     必须等待 offset 提交完毕,再去消费下一批数据.
    - commitAsync(异步提交):    发送玩提交 offset 请求后,就开始消费下一批数据了.

10.指定 offset 消费
    - auto.offset.reset = earliest|latest|none, 默认是 latest
    - 当 Kafka 中没有偏移量(消费者组第一次消费)或服务器上不再存在当前偏移量时(例如该数据已被删除),该怎么办?
    - earliest: 自动将偏移量重置为最早的偏移量, --from-beginning
      latest(默认值): 自动将偏移量重置为最新偏移量
      none: 如果未找到消费者的先前偏移量, 则向消费者抛出异常

11.按照时间消费
12.消费者事务
    1.重复消费: 已经消费了,但是 offset 没提交.
      - consumer 每5秒提交 offset
      - 如果提交 offset 后的2s, consumer 挂了
      - 再次重启 consumer, 则从上一次提交的 offset 处继续消费,导致重复消费
    2.漏消费: 先提交 offset 后消费,有可能会造成数据的漏消费.
      - 设置 offset 为手动提交, 当 offset 提交时,数据还在内存中未落盘,此时刚好消费者线程
        被 kill 掉, 那么 offset 已经提交,但是数据未处理,导致这部分内存中的数据丢失.
      - 提交 offset, 消费者消费的数据还在内存中,消费者挂掉,导致漏消费.

13.数据积压(消费者如何提高吞吐量)
    1.如果是 kafka 消费能力不足,则可以考虑增加 Topic 的分区数,并且同时提升消费组的消费者
      数量. 消费者=分区数. 两者缺一不可.
    2.如果是下游的数据处理不及时: 提高每批次拉取的数量. 批次拉取数据过少(拉取数据处理时间 < 生产速度),
      使处理的数据小于生产的数据. 也会造成数据积压.
